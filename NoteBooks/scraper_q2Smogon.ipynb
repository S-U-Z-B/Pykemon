{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import bs4 as bs\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from IPython import embed\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "website = 'https://www.smogon.com/stats/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the following two functions is to either download the html of a website page, or to open the website using selenium. In the case of websites which require dynamically generated content (without the url change. Javascript for example), selenium is required. In our case it is not required for Smogon, since every url change means a page change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The directory for \"geckodriver\" must be specified to the location of the file within the submission folder to allow the scraper to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(page, addition=''):\n",
    "    return bs.BeautifulSoup(urlopen(Request(page + addition,\n",
    "            headers={'User-Agent': 'Opera/9.80 (X11; Linux i686; Ub'\\\n",
    "                     'untu/14.10) Presto/2.12.388 Version/12.16'})).read(), 'lxml')\n",
    "def fetch_sel(page, headless = True):\n",
    "\toptions = webdriver.FirefoxOptions()\n",
    "\tif headless:\n",
    "\t\toptions.add_argument('-headless')\n",
    "\tdirect = os.path.dirname(__file__)\n",
    "\texe_path = os.path.join(direct, 'geckodriver')\n",
    "\texe_path = '/Users/ajkea/Documents/geckodriver'\n",
    "\tdriver = webdriver.Firefox(executable_path=exe_path,options=options)\n",
    "\tdriver.get(page)\n",
    "\t\n",
    "\thtml = driver.page_source\n",
    "\treturn bs.BeautifulSoup(html,'lxml'), driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is the main scraping function. Here are the steps to get the data.\n",
    "\n",
    "1. Create empty dataframes, and list of formats (used later)\n",
    "2. The main scraping loop.\n",
    "    1. Loop through every date/year where the information is available. We can then add it to the url and fetch the new url. For example, the base url is: https://www.smogon.com/stats/ and the url with dates is https://www.smogon.com/stats/2014-11/.\n",
    "    2. For each of the months, we then loop through every available file and find the last available for each of the leagues. This is done quite inneficiently since we need to loop through the list of available files many times to check if there are more than one file for a certain league.\n",
    "    3. Once we know which files to gather, we can use pandas to read the csv (txt file) with pd.read_csv(https://www.smogon.com/stats/2014-11/randombattle-1500.txt) for example.\n",
    "    4. The information is then added to the full pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smogon_stats(website):\n",
    "\t''' Gets all the monthly txt files'''\n",
    "\tdf_metadata = pd.DataFrame(columns=['txt_file','tot_battles','avg_weight_team'])\n",
    "\tdf_poke_data = pd.DataFrame(columns=['file_index','pokemon','usage_perc','raw','perc','real2','perc2'])\n",
    "\t\n",
    "\tformats = [\"ubers-\",\"ou-\",\"uu-\",\"ru-\",\"pu-\",\"vgc20\"]\n",
    "\n",
    "\tresult_page = fetch(f'{website}')\n",
    "\n",
    "\tyr_dates = result_page.find('pre')\n",
    "\tfor yr_dt in yr_dates.find_all('a'):\n",
    "\t\tdate_href = yr_dt['href']\n",
    "\t\tprint(date_href)\n",
    "\t\tif len(date_href.split('-')) == 2:\n",
    "\t\t\tformats_done = []\n",
    "\t\t\tresult_page_date = fetch(f'{website}{date_href}').find('pre')\n",
    "\t\t\tall_txt_files = [x['href'] for x in result_page_date.find_all('a')]\n",
    "\t\t\tlast_gen_val = 0\n",
    "\t\t\tfor file in all_txt_files:\n",
    "\t\t\t\tif file[:3] == 'gen':\n",
    "\t\t\t\t\tlast_gen = file[3]\n",
    "\t\t\t\t\tif last_gen!= 'e' and last_gen.isnumeric() and int(last_gen) > last_gen_val:\n",
    "\t\t\t\t\t\tlast_gen_val = int(last_gen)\n",
    "\t\t\tfor format_ in formats:\n",
    "\t\t\t\tgen_files = [x for x in all_txt_files if f\"{format_}\" in x]\n",
    "\t\t\t\tif len(gen_files) == 4:\n",
    "\t\t\t\t\t# Only available ones\n",
    "\t\t\t\t\tpass\n",
    "\t\t\t\telif len(gen_files) < 4:\n",
    "\t\t\t\t\tprint(f\"1-Problem with {format_}, yr_dt:{date_href}, len:{len(gen_files)} \\n\\t\\tFiles: {gen_files}\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# print(gen_files)\n",
    "\t\t\t\t\t# print(f'gen{last_gen}')\n",
    "\t\t\t\t\t# Fil to be put in df\n",
    "\t\t\t\t\tgen_files2 = [x for x in gen_files if f'gen{last_gen_val}{format_}' in x]\n",
    "\t\t\t\t\tif format_ == 'vgc20' and len(gen_files2) != 4:\n",
    "\t\t\t\t\t\tgen_files2 = gen_files[-4:]\n",
    "\t\t\t\t\tif f\"{format_}0.txt\" in gen_files and len(gen_files2) != 4:\n",
    "\t\t\t\t\t\tgen_files2 = [x for x in gen_files if x[:2] == format_[:2]]\n",
    "\t\t\t\t\tif len(gen_files2) != 4 and len(gen_files) != 4:\n",
    "\t\t\t\t\t\tprint(f\"2-Problem with {format_}, yr_dt:{date_href}, len:{len(gen_files)} \\n\\t\\tFiles: {gen_files}\\n\\t\\tFiles: {gen_files2}\\n\\t\\tgen{last_gen_val}{format_}\")\n",
    "\t\t\t\t\t\tgen_files2 = gen_files[-4:]\n",
    "\n",
    "\t\t\t\tfor txt_file_name in gen_files2:\n",
    "\t\t\t\t\t# print(txt_file_name)\n",
    "\t\t\t\t\t# file = urllib.request.urlopen(f\"{website}{date_href}{txt_file_name}\")\n",
    "\t\t\t\t\tfilename = f\"{website}{date_href}{txt_file_name}\"\n",
    "\t\t\t\t\tfile = requests.get(filename).text.splitlines()[:2]\n",
    "\t\t\t\t\ttot_battles = int(float(file[0].split(':')[-1].strip()))\n",
    "\t\t\t\t\tavg_weight_team = float(file[1].split(':')[-1].strip())\n",
    "\n",
    "\t\t\t\t\tdf_metadata = df_metadata.append({'txt_file':filename,'tot_battles':tot_battles,\n",
    "\t\t\t\t\t\t\t\t\t\t\t'avg_weight_team':avg_weight_team}, ignore_index=True)\n",
    "\n",
    "\t\t\t\t\tdf = pd.read_csv(filename, skiprows=[0,1,2,4], delimiter='|')\n",
    "\t\t\t\t\tdf.drop(df.index[-1],inplace=True)\n",
    "\t\t\t\t\tremove_cols = list(df.columns[:2])+list(df.columns[-1:])\n",
    "\t\t\t\t\tdf.drop(remove_cols,axis=1,inplace=True)\n",
    "\t\t\t\t\tdf.columns = ['pokemon','usage_perc','raw','perc','real2','perc2']\n",
    "\t\t\t\t\tdf['file_index'] = max(df_metadata.index)\n",
    "\t\t\t\t\tdf_poke_data = df_poke_data.append(df, ignore_index=True)\n",
    "\t\n",
    "\n",
    "\tperc_cols = [x for x in df_poke_data.columns if 'perc' in x]\n",
    "\tfor col in perc_cols:\n",
    "\t\tdf_poke_data[col] = df_poke_data[col].apply(lambda x: float(x.replace('%',''))/100)\n",
    "\n",
    "\tdf_metadata.to_csv('metadata.csv')\n",
    "\tdf_poke_data.to_csv('pokemon_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_smogon_stats(website)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
